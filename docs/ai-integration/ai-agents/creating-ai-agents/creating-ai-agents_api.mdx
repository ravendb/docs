---
title: "Creating AI agents: API"
hide_table_of_contents: true
sidebar_label: Client API
sidebar_position: 1
---

import Admonition from '@theme/Admonition';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';
import LanguageSwitcher from "@site/src/components/LanguageSwitcher";
import LanguageContent from "@site/src/components/LanguageContent";

# Creating AI agents: API
<Admonition type="note" title="">

* To create an AI agent, a client defines its configuration, provides it with settings and tools, and registers the agent with the server.  

* Once the agent is created, the client can initiate or resume conversations, get LLM responses, and perform actions based on LLM insights.  

* This page provides a step-by-step guide to creating an AI agent and interacting with it using the Client API.  

* In this article:
   * [Creating a connection string](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#creating-a-connection-string)  
   * [Defining an agent configuration](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#defining-an-agent-configuration)
      * [Set the agent ID](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#set-the-agent-id)
      * [Add agent parameters](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#add-agent-parameters)
      * [Set maximum number of iterations](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#set-maximum-number-of-iterations)
      * [Set chat trimming configuration](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#set-chat-trimming-configuration)
   * [Adding agent tools](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#adding-agent-tools)
      * [Query tools](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#query-tools)
         * [Initial context queries](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#initial-context-queries)
      * [Action tools](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#action-tools)
   * [Creating a Response object and the Agent](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#creating-a-response-object-and-the-agent)
   * [Managing conversations](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#managing-conversations)
      * [Setting conversation](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#setting-conversation)
      * [Creating action-tool handlers](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#creating-action-tool-handlers)
      * [Conversation response](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#conversation-response)
      * [Setting user prompt and running conversation](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#setting-user-prompt-and-running-conversation)
   * [Full Example](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#full-example)
   * [Stream LLM responses](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#stream-llm-responses)

</Admonition>

## Creating a connection string

Your agent will need a connection string to connect with the LLM. Create a connection string using an `AiConnectionString` instance and the `PutConnectionStringOperation` operation.  
(You can also create a connection string using Studio, see [here](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_studio#configure-basic-settings))

You can use a local `Ollama` model if your considerations are mainly speed, cost, open-source, or security,  
Or you can use a remote `OpenAI` service for its additional resources and capabilities.  

* **Example**  
  <Tabs groupId='languageSyntax'>
  <TabItem value="open-ai-cs" label="open-ai-cs">
  ```csharp
  using (var store = new DocumentStore())
  {
    // Define the connection string to OpenAI
    var connectionString = new AiConnectionString
    {
        // Connection string name & identifier
        Name = "open-ai-cs",

        // Connection type
        ModelType = AiModelType.Chat,

        // OpenAI connection settings
        OpenAiSettings = new OpenAiSettings(
            apiKey: "your-api-key",
            endpoint: "https://api.openai.com/v1",
            // LLM model for text generation
            model: "gpt-4.1")
    };

    // Deploy the connection string to the server
    var operation = new PutConnectionStringOperation<AiConnectionString>(connectionString);
    var putConnectionStringResult = store.Maintenance.Send(operation);
  }
  ```
  </TabItem>
  <TabItem value="ollama-cs" label="ollama-cs">
  ```csharp
  using (var store = new DocumentStore())
  {
    // Define the connection string to Ollama
    var connectionString = new AiConnectionString
    {
        // Connection string name & identifier
        Name = "ollama-cs",

        // Connection type
        ModelType = AiModelType.Chat,

        // Ollama connection settings
        OllamaSettings = new OllamaSettings(
            // LLM Ollama model for text generation
            model: "llama3.2",
            // local URL
            uri: "http://localhost:11434/")
    };

    // Deploy the connection string to the server
    var operation = new PutConnectionStringOperation<AiConnectionString>(connectionString);
    var putConnectionStringResult = store.Maintenance.Send(operation);
  }
  ```
  </TabItem>
  </Tabs>

* **Syntax**  
  <Tabs groupId='languageSyntax'>
  <TabItem value="open-ai-cs-syntax" label="open-ai-cs-syntax">
  ```csharp
  public class AiConnectionString
  {
    public string Name { get; set; }
    public AiModelType ModelType { get; set; }
    public string Identifier { get; set; }
    public OpenAiSettings OpenAiSettings { get; set; }
    ...
  }

  public class OpenAiSettings : AbstractAiSettings
  {
    public string ApiKey { get; set; }
    public string Endpoint { get; set; }
    public string Model { get; set; }
    public int? Dimensions { get; set; }
    public string OrganizationId { get; set; }
    public string ProjectId { get; set; }
  }
  ```
  </TabItem>
  <TabItem value="ollama-cs-syntax" label="ollama-cs-syntax">
  ```csharp
  public class AiConnectionString
  {
    public string Name { get; set; }
    public AiModelType ModelType { get; set; }
    public string Identifier { get; set; }
    public OllamaSettings OllamaSettings { get; set; }
    ...
  }

  public class OllamaSettings : AbstractAiSettings
  {
    public string Model { get; set; }
    public string Uri { get; set; }
  }
  ```
  </TabItem>
  </Tabs>

## Defining an agent configuration

To create an AI agent you need to prepare an **agent configuration** and populate it with 
your settings and tools.  

Start by creating a new `AiAgentConfiguration` instance.  
While creating the instance, pass its constructor:  

- The agent's Name  
- The [connection string](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#creating-a-connection-string) you created  
- A System prompt  

The agent will send the system prompt you define here to the LLM to define its basic characteristics, including its role, purpose, behavior, and the tools it can use.  

* **Example**  
  ```csharp
  // Start setting an agent configuration
  var agent = new AiAgentConfiguration("reward-productive-employee", connectionString.Name,
  @"You work for a human experience manager.
  The manager uses your services to find which employee has made the largest profit and to suggest 
  a reward.
  The manager provides you with the name of a country, or with the word ""everything"" to indicate 
  all countries.
  Then you:
  1. use a query tool to load all the orders sent to the selected country,
  or a query tool to load all orders sent to all countries.
  2. calculate which employee made the largest profit.
  3. use a query tool to learn in what general area this employee lives.
  4. find suitable vacations sites or other rewards based on the employee's residence area.
  5. use an action tool to store in the database the employee's ID, profit, and your reward suggestions.
  When you're done, return these details in your answer to the user as well.");
  ```

* `AiAgentConfiguration` constructor  
  ```csharp
  public AiAgentConfiguration(string name, string connectionStringName, string systemPrompt);
  ```

* `AiAgentConfiguration` class  
  ```csharp
  public class AiAgentConfiguration
  {
    // A unique identifier given to the AI agent configuration
    public string Identifier { get; set; }

    // The name of the AI agent configuration
    public string Name { get; set; }

    // Connection string name
    public string ConnectionStringName { get; set; }

    // The system prompt that defines the role and purpose of the agent and the LLM
    public string SystemPrompt { get; set; }

    // An example object that sets the layout for the LLM's response to the user.
    // The object is translated to a schema before it is sent to the LLM.
    public string SampleObject { get; set; }

    // A schema that sets the layout for the LLM's response to the user.
    // If both a sample object and a schema are defined, only the schema is used.
    public string OutputSchema { get; set; }

    // A list of Query tools that the LLM can use (through the agent) to access the database
    public List<AiAgentToolQuery> Queries { get; set; } = new List<AiAgentToolQuery>();

    // A list of Action tools that the LLM can use to trigger the user to action
    public List<AiAgentToolAction> Actions { get; set; } = new List<AiAgentToolAction>();

    // Agent parameters whose value the client passes to the LLM each time a chat is started, 
    // for stricter control over queries initiated by the LLM and as a means for interaction 
    // between the client and the LLM.  
    public List<AiAgentParameter> Parameters { get; set; } = new List<AiAgentParameter>();

    // The trimming configuration defines if and how the chat history is summarized, 
    // to minimize the amount of data passed to the LLM when a chat is started.  
    public AiAgentChatTrimmingConfiguration ChatTrimming { get; set; } = new  
        AiAgentChatTrimmingConfiguration(new AiAgentSummarizationByTokens());
    
    // Control over the number of times that the LLMis allowed to use agent tools to handle 
    // a user prompt.  
    public int? MaxModelIterationsPerCall { get; set; }
  }
  ```

Once the initial agent configuration is created, we need to add it a few additional elements.  

### Set the agent ID:
Use the `Identifier` property to provide the agent with a unique ID that the 
system will recognize it by.  

```csharp
// Set agent ID
agent.Identifier = "reward-productive-employee";
```

### Add agent parameters:
Agent parameters are parameters that can be used by [query tools](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#query-tools) when the agent queries the database on behalf of the LLM.  
Values for agent parameters are provided by the client, or by a user through the client, 
when a chat is started.  
When the agent is requested to use a query tool that uses agent parameters, it replaces these parameters with the values provided by the user before running the query.  
Using agent parameters allows the client to focus the queries and the entire interaction on its current needs.  

In the example below, an agent parameter is used to determine what area 
of the world a query will handle. 

To add an agent parameter create an `AiAgentParameter` instance, initialize it with 
the parameter's **name** and **description** (explaining to the LLM what the parameter 
is for), and pass this instance to the `agent.Parameters.Add` method.  

* **Example**  
  ```csharp
  //  Set agent parameters
  agent.Parameters.Add(new AiAgentParameter(
      "country", "A specific country that orders were shipped to, " +
      "or \"everywhere\" to look for orders shipped to all countries"));
  ```

* `AiAgentParameter` Definition 
  ```csharp
  public AiAgentParameter(string name, string description);
  ```

### Set maximum number of iterations:
You can limit the number of times that the LLM is allowed to request the usage of 
agent tools in response to a single user prompt. Use `MaxModelIterationsPerCall` to change this limit.  

* **Example**  
  ```csharp
  // Limit the number of times the LLM can request for tools in response to a single user prompt
  agent.MaxModelIterationsPerCall = 3;
  ```

* `AiAgentParameter` Definition  
  ```csharp
  public int? MaxModelIterationsPerCall
  ```

<Admonition type="note" title="">
Note that if limiting the amount of time a user needs to wait for the LLM to respond is a main consideration, you can also stream the LLM's response to the client. Find more about streaming in the [overview](../../../ai-integration/ai-agents/ai-agents_overview#streaming-llm-responses) and [below](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#stream-llm-responses).  
</Admonition>

### Set chat trimming configuration:

To [summarize the conversation history](../../../ai-integration/ai-agents/ai-agents_overview#define-a-chat-trimming-configuration), create an `AiAgentChatTrimmingConfiguration` instance, 
use it to configure your trimming strategy, and set the agent's `ChatTrimming` property 
with the instance.  

When creating the instance, pass its constructor a summarization strategy using 
a `AiAgentSummarizationByTokens` class.  

The original conversation history, before it was summarized, can optionally be 
kept in the `@conversations-history` collection.  
To determine whether to keep the original messages and for how long, also pass the 
`AiAgentChatTrimmingConfiguration` constructor an `AiAgentHistoryConfiguration` instance 
with your history settings.  

* **Example**  
  ```csharp
  // Set chat trimming configuration
  AiAgentSummarizationByTokens summarization = new AiAgentSummarizationByTokens()
  {
    // When the number of tokens stored in the conversation exceeds this limit
    // summarization of old messages will be triggered.
    MaxTokensBeforeSummarization = 32768,
    // The maximum number of tokens that the conversation is allowed to contain
    // after summarization. 
    MaxTokensAfterSummarization = 1024
  };
  agent.ChatTrimming = new AiAgentChatTrimmingConfiguration(summarization);
  ```

* **Syntax**  
  ```csharp
  public class AiAgentSummarizationByTokens
  {
    // The maximum number of tokens allowed before summarization is triggered.
    public long? MaxTokensBeforeSummarization { get; set; }

    // The maximum number of tokens allowed in the generated summary.
    public long? MaxTokensAfterSummarization { get; set; }
  }

  public class AiAgentHistoryConfiguration
  {
    // Enables history for the AI agents conversations.
    public AiAgentHistoryConfiguration()

    // Enables history for the AI agents conversations.
    // <param name="expiration">The timespan after which history documents expire.</param>
    public AiAgentHistoryConfiguration(TimeSpan expiration)

    // The timespan after which history documents expire.
    public int? HistoryExpirationInSec { get; set; }
  }
  ```

## Adding agent tools

* You can enhance your agent with Query and Action tools, that allow the LLM 
  to query your database and trigger client actions, respectively  
* After defining agent tools and submitting them to the LLM, it is up to the LLM 
  to decide if and when to use them. 

### Query tools:

[Query tools](../../../ai-integration/ai-agents/ai-agents_overview#query-tools) provide the LLM with the ability to retrieve data from the database.  
A query tool includes a natural-language **description** that explains the LLM what the tool is for, and an **RQL query**.  

* **Passing values to query tools**  
   * Query tools optionally include [parameters](../../../ai-integration/ai-agents/ai-agents_overview#query-parameters), identified by a `$` prefix.  
     Both the user and the LLM can pass values to these parameters.  
   * **Passing values from the user**  
     Users can pass values to queries through **agent parameters**.  
     If agent parameters are defined in the agent configuration -  
       * The client has to provide values for them when initiating a conversation with the agent.  
       * The parameters can be included in query tools RQL queries.  
     Before running a query, the agent will replace any agent parameter included in it with its value.  
   * **Passing values from the LLM**  
     The LLM can pass values to queries through a **parameters schema**.  
      * The parameters schema layout is defined as part of the query tool.  
      * When the LLM requests the agent to run a query, it will add parameter values to the request.  
      * You can define a parameters schema either as a **sample object** or a **formal JSON schema**.  
        If you define both, the LLM will pass parameter values only through the JSON schema.  
      * Before running a query, the agent will replace any parameter included in it with its value.  

* **Example**  
   * The first query tool will be used by the LLM when it needs to retrieve all the 
     orders sent to any place in the world. (the system prompt instructs it to use this 
     tool when the user enters "everywhere" when the conversation is started.)  
   * The second query tool will be used by the LLM when it needs to retrieve all the 
     orders that were sent to a particular country, using the `$country` agent parameter.  
   * The third tool retrieves from the database the general location of an employee.  
     To do this it uses a `$employeeId` parameter, whose value is set by the LLM in its 
     request to run this tool.  

          ```csharp
          agent.Queries =
          [
            // Set a query tool that triggers the agent to retrieve all the orders sent everywhere
            new AiAgentToolQuery
            {
                // Query tool name
                Name = "retrieve-orders-sent-to-all-countries",

                // Query tool description
                Description = "a query tool that allows you to retrieve all orders sent to all countries.",

                // Query tool RQL query
                Query = "from Orders as O select O.Employee, O.Lines.Quantity",

                // Sample parameters object for the query tool
                // The LLM can use this object to pass parameters to the query tool
                ParametersSampleObject = "{}"
            },
        
            // Set a query tool that triggers the agent to retrieve all the orders sent to a 
            // specific country
            new AiAgentToolQuery
            {
                Name = "retrieve-orders-sent-to-a-specific-country",
                Description = "a query tool that allows you to retrieve all orders sent " + 
                "to a specific country",
                Query = "from Orders as O where O.ShipTo.Country == $country select O.Employee, " + 
                "O.Lines.Quantity",
                ParametersSampleObject = "{}"
            },

            // Set a query tool that triggers the agent to retrieve the performer's
            // residence region details (country, city, and region) from the database
            new AiAgentToolQuery
            {
                Name = "retrieve-performer-living-region",
                Description = "a query tool that allows you to retrieve an employee's country, " + 
                "city, and region, by the employee's ID",
                Query = "from Employees as E where id() == $employeeId select E.Address.Country, " + 
                "E.Address.City, E.Address.Region",
                ParametersSampleObject = "{" +
                              "\"employeeId\": \"embed the employee's ID here\"" +
                                         "}"
            }
          ];
          ```

* **Syntax**  
  Query tools are defined in a list of `AiAgentToolQuery` classes.  
  ```csharp
  public class AiAgentToolQuery
  {
      public string Name { get; set; }
      public string Description { get; set; }
      public string Query { get; set; }
      public string ParametersSampleObject { get; set; }
      public string ParametersSchema { get; set; }
  }
  ```

#### Initial context queries:

* You can set a query tool as an [initial context query](../../../ai-integration/ai-agents/ai-agents_overview#initial-context-queries) using its `Options.AddToInitialContext` property, to execute the query immediately when the agent is started and provide the LLM with the query results as part of the initial context.  
   * An initial context query is **not allowed** to use LLM parameters, since the LLM 
     will not have the opportunity to fill the parameters with values before the query is executed.  
   * The query **can** use agent parameters, whose values are provided by the user when the conversation is started.  

*  You can also Enable or Disable a query tool using the `Options.AllowModelQueries` property.  
   * When enabled, the LLM can freely trigger the execution of this query tool.  
   * When disabled, the LLM cannot trigger the execution of this query tool.  
   * If the tool is set as an initial context query, it will be executed when the conversation 
     starts even if disabled using `AllowModelQueries`.  

* **Example**  
  Set a query tool that runs when the agent is started and retrieves all the orders sent everywhere.
  ```csharp
  new AiAgentToolQuery
  {
     Name = "retrieve-orders-sent-to-all-countries",
     Description = "a query tool that allows you to retrieve all orders sent to all countries.",
     Query = "from Orders as O select O.Employee, O.Lines.Quantity",
     ParametersSampleObject = "{}"

     Options = new AiAgentToolQueryOptions
     {
         // The LLM is allowed to trigger the execution of this query during the conversation
         AllowModelQueries = true,

         // The query will be executed when the conversation starts 
         // and its results will be added to the initial context
         AddToInitialContext = true
      }
  }
  ```

* **Syntax**  
  ```csharp
  public class AiAgentToolQueryOptions : IDynamicJson
  {
      public bool? AllowModelQueries { get; set; }
      public bool? AddToInitialContext { get; set; }
  }
    ```

      |Property|Type|Description|
      |--------|----|-----------|
      |`AllowModelQueries`|`bool`| `true`: the LLM can trigger the execution of this query tool.<br />`false`: the LLM cannot trigger the execution of this query tool.<br />`null`: server-side defaults apply.|
      |`AddToInitialContext`|`bool`| `true`: the query will be executed when the conversation starts and its results added to the initial context.<br />`false`: the query will not be executed when the conversation starts.<br />`null`: server-side defaults apply.|

<Admonition type="note" title="">
The two flags can be set regardless of each other.  
* Setting `AddToInitialContext` to `true` and `AllowModelQueries` to `false`  
  will cause the query to be executed when the conversation starts,  
  but the LLM will not be able to trigger its execution later in the conversation.  
* Setting `AddToInitialContext` to `true` and `AllowModelQueries` to `true`  
  will cause the query to be executed when the conversation starts,  
  and the LLM will also be able to trigger its execution later in the conversation.  
</Admonition>

### Action tools:

Action tools allow the LLM to trigger the client to action (e.g., to modify or add a document).  
An action tool includes a natural-language **description** that explains the LLM what the tool is capable of, and a **schema** that the LLM will fill with details related to the requested action before sending it to the agent.  

In the example below, the action tool requests the client to store an employee's details 
in the database. The LLM will provide the employee's ID and other details whenever it requests the agent 
to apply the tool. 

When the client finishes performing the action, it is required to send the LLM 
a response that explains how it went, e.g. `done`.  

* **Example**  
  The following action tool sends to the client employee details that the tool needs to store in the database.  
  ```csharp
  agent.Actions =
  [
    // Set an action tool that triggers the client to store the performer's details
    new AiAgentToolAction
        {
            Name = "store-performer-details",
            Description = "an action tool that allows you to store the ID of the employee that made " +
                          "the largest profit, the profit, and your suggestions for a reward, in the " + 
                          "database.",
            ParametersSampleObject = "{" +
                        "\"suggestedReward\": \"embed your suggestions for a reward here\", " +
                        "\"employeeId\": \"embed the employee’s ID here\", " +
                        "\"profit\": \"embed the employee’s profit here\"" +
                                         "}"
        }
  ];
  ```

* **Syntax**  
  Action tools are defined in a list of `AiAgentToolAction` classes.  
  ```csharp
  public class AiAgentToolAction
  {
    public string Name { get; set; }
    public string Description { get; set; }
    public string ParametersSampleObject { get; set; }
    public string ParametersSchema { get; set; }
  }
  ```

## Creating a Response object and the Agent

The agent configuration is almost ready.  
The only element still missing is an object for the LLM's response, 
when the LLM finishes its work and needs to reply.  

Create a response object class with the fields that you want the LLM to fill in its response.  
Then, create the agent using the `CreateAgentAsync` method and pass it a new 
instance of your response object.  
Set each response-object property with a natural-language explanation to the LLM, indicating 
what the LLM should embed in it.

* **Example**  
  ```csharp
  // Create the agent
  // Pass it an object for its response
  var createResult = await store.AI.CreateAgentAsync(agent, new Performer
  {
    suggestedReward = "your suggestions for a reward",
    employeeId = "the ID of the employee that made the largest profit",
    profit = "the profit the employee made"
  });

  // An object for the LLM response
  public class Performer
  {
     public string suggestedReward;
     public string employeeId;
     public string profit;
  }
  ```

* Alternatively, you can set the response object as part of the agent configuration, like so -  
  ```csharp
  // Set sample object
  agent.SampleObject = "{" +
                    "\"suggestedReward\": \"your suggestions for a reward\", " +
                    "\"employeeId\": \"the ID of the employee that made the largest profit\", " +
                    "\"profit\": \"the profit the employee made\"" +
                          "}";
  ```
  If you apply this option, make sure you create the agent using a `CreateAgentAsync` overload that doesn't require you to pass it a response object.  

* `CreateAgentAsync` overloads  
  ```csharp
  // Create the agent with just the defined configuration
  CreateAgentAsync(configuration);

  // Create the agent with just the defined configuration
  CreateAgentAsync(AiAgentConfiguration configuration, CancellationToken token = default(CancellationToken));

  // Create the agent while passing it a response object
  CreateAgentAsync<TSchema>(AiAgentConfiguration configuration, TSchema sampleObject, CancellationToken token = default(CancellationToken));
  ```

<hr />

## Managing conversations

### Setting conversation:

* Set a conversation using the `store.AI.Conversation` method.  
  Pass `Conversation`:  
   * The **agent ID**  
   * The **conversation ID**  
      * Conversations are kept in the `@conversations` collection.  
        A conversation document's name starts with a prefix (such as `Chats/`) that can be 
        set when the conversation is initiated.  
      * You can -  
        **Provide a full ID**, including a prefix and the ID that follows it.  
        **Provide a prefix that ends with `/` or `|`** to trigger automatic ID creation, 
        similarly to the creation of automatic IDs for documents.  
      * If you pass the method the ID of an existing conversation (e.g. `"Chats/0000000000000008883-A"`) 
        the conversation will be retrieved from storage and continued where you left off.  
      * If you provide an empty prefix (e.g. `"Chats/`), a new conversation will start.  
   * Values for **agent parameters**, if defined, in an `AiConversationCreationOptions` instance.  
* Set the user prompt using the `SetUserPrompt`method.  
  The user prompt informs the agent of the user's requests and expectations for this chat.  
* Use the value returned by the `Conversation` method to run the chat.

* **Example**  
  ```csharp
  // Create a conversation instance
  // Initialize it with - 
  // The agent's ID, 
  // A prefix (Performers/) for conversations stored in the @Conversations collection,
  // Agent parameters' values
  var chat = store.AI.Conversation(
      createResult.Identifier,
      "Performers/",
      new AiConversationCreationOptions().AddParameter("country", "France"));
  ```

* `Conversation` Definition  
  ```csharp
  public IAiConversationOperations Conversation(string agentId, string conversationId, AiConversationCreationOptions creationOptions, string changeVector = null)
  ```

* `SetUserPrompt` Definition  
  ```csharp
  void SetUserPrompt(string userPrompt);
  ```

### Creating action-tool handlers:

Handle LLM action requests by creating a handler for each action tool using the conversation's `Handle` method.  
Pass `Handle` -  
 * The action tool's name.  
 * An object to populate with the data sent with each action request.  
   Make sure the object has the same structure you defined for the action tool's parameters schema.  

When you finish handling the requested action, `return` the LLM an indication that it was done.  

* **Example**  
  In this example, the action tool is requested to store an employee's details in the database.  
  ```csharp
  // "store-performer-details" action tool handler
  chat.Handle("store-performer-details", (Performer performer) =>
  {
      using (var session = store.OpenSession())
      {
          // These values are passed to the client by the action tool
          Performer rewarded = new Performer
          {
              suggestedReward = performer.suggestedReward,
              employeeId = performer.employeeId,
              profit = performer.profit
          };

          // store the values in the Performers collection in the database
          session.Store(rewarded);
          session.SaveChanges();
      }

      // return to the agent an indication that the action went well.
      return "done";
  });

  // An object for the LLM response
  public class Performer
  {
      public string suggestedReward;
      public string employeeId;
      public string profit;
  }
  ```

### Conversation response:

The LLM response is returned by the agent to the client in an `AiAnswer` object.  
The conversation status is indicated by `AiAnswer.AiConversationResult`.   

* `AiAnswer`syntax  
  ```csharp
  public class AiAnswer<TAnswer>
  {
      // The answer content produced by the AI
      public TAnswer Answer;

      // The status of the conversation
      public AiConversationResult Status;
  }

  public enum AiConversationResult
  {
      // The conversation has completed and a final answer is available
      Done,
      // Further interaction is required, such as responding to tool requests
      ActionRequired
  }
  ```

### Setting user prompt and running conversation:

Set the user prompt using the `SetUserPrompt` method, and run the conversation using the
`RunAsync` method.
<Admonition type="note" title="">
You can also use `StreamAsync` to **stream** the LLM's response as it is generated.  
Learn how to do this in the [Stream LLM responses](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#stream-llm-responses) section.
</Admonition>

```csharp
// Set the user prompt and run the conversation
chat.SetUserPrompt("send a few suggestions to reward the employee that made the largest profit");

var LLMResponse = await chat.RunAsync<Performer>(CancellationToken.None);

if (LLMResponse.Status == AiConversationResult.Done)
{
    // The LLM successfully processed the user prompt and returned its response.
    // The performer's ID, profit, and suggested rewards were stored in the Performers
    // collection by the action tool, and are also returned in the final LLM response.
}
```

<hr />

## Full example

The agent's user in this example is a human experience manager.  
The agent helps its user to reward employees by searching, using query tools, 
for orders sent to a certain country or (if the user prompts it "everywhere") 
to all countries, and finding the employee that made the largest profit.  
The agent then runs another query tool to find, by the employee's ID (that 
was fetched from the retrieved orders) the employee's residence region, 
and finds rewards suitable for the employee based on this region.  
Finally, it uses an action tool to store the employee's ID, profit, and reward 
suggestions in the `Performers` collection in the database, and returns the same 
details in its final response as well.  

```csharp
public async Task createAndRunAiAgent_full()
{
    var store = new DocumentStore();

    // Define connection string to OpenAI
    var connectionString = new AiConnectionString
    {
        Name = "open-ai-cs",
        ModelType = AiModelType.Chat,
        OpenAiSettings = new OpenAiSettings(
            apiKey: "your-api-key",
            endpoint: "https://api.openai.com/v1",
            // LLM model for text generation
            model: "gpt-4.1")
    };

    // Deploy connection string to server
    var operation = new PutConnectionStringOperation<AiConnectionString>(connectionString);
    var putConnectionStringResult = store.Maintenance.Send(operation);

    using var session = store.OpenAsyncSession();

    // Start setting an agent configuration
    var agent = new AiAgentConfiguration("reward-productive-employee", connectionString.Name,
    @"You work for a human experience manager.
    The manager uses your services to find which employee has made the largest profit and to suggest 
    a reward.
    The manager provides you with the name of a country, or with the word ""everything"" to indicate 
    all countries.
    Then you:
    1. use a query tool to load all the orders sent to the selected country,
    or a query tool to load all orders sent to all countries.
    2. calculate which employee made the largest profit.
    3. use a query tool to learn in what general area this employee lives.
    4. find suitable vacations sites or other rewards based on the employee's residence area.
    5. use an action tool to store in the database the employee's ID, profit, and your reward suggestions.
    When you're done, return these details in your answer to the user as well.");

    // Set agent ID
    agent.Identifier = "reward-productive-employee";

    //  Set agent parameters
    agent.Parameters.Add(new AiAgentParameter(
                                  "country", "A specific country that orders were shipped to, " +
                                  "or \"everywhere\" to look for orders shipped to all countries"));

    agent.Queries =
    [
        // Set a query tool to retrieve all orders sent everywhere
        new AiAgentToolQuery
            {
                // Query tool name
                Name = "retrieve-orders-sent-to-all-countries",

                // Query tool description
                Description = "a query tool that allows you to retrieve all orders sent to all countries.",

                // Query tool RQL query
                Query = "from Orders as O select O.Employee, O.Lines.Quantity",

                // Sample parameters object 
                ParametersSampleObject = "{}"
            },
        
            // Set a query tool to retrieve all orders sent to a specific country
            new AiAgentToolQuery
            {
                Name = "retrieve-orders-sent-to-a-specific-country",
                Description = 
                       "a query tool that allows you to retrieve all orders sent to a specific country",
                Query = 
                       "from Orders as O where O.ShipTo.Country == " + 
                       "$country select O.Employee, O.Lines.Quantity",
                ParametersSampleObject = "{}"
            },

            // Set a query tool to retrieve the performer's residence region details from the database
            new AiAgentToolQuery
            {
                Name = "retrieve-performer-living-region",
                Description = 
                        "a query tool that allows you to retrieve an employee's country, city, and " + 
                        "region, by the employee's ID",
                Query = "from Employees as E where id() == $employeeId select E.Address.Country, " + 
                        "E.Address.City, E.Address.Region",
                ParametersSampleObject = "{" +
                          "\"employeeId\": \"embed the employee's ID here\"" +
                         "}"
            }
    ];

    agent.Actions =
    [
        // Set an action tool to store the performer's details
        new AiAgentToolAction
            {
                Name = "store-performer-details",
                Description = 
                    "an action tool that allows you to store the ID of the employee that made " +
                    "the largest profit, the profit, and your suggestions for a reward, in the database.",
                ParametersSampleObject = "{" +
                          "\"suggestedReward\": \"embed your suggestions for a reward here\", " +
                          "\"employeeId\": \"embed the employee’s ID here\", " +
                          "\"profit\": \"embed the employee’s profit here\"" +
                               "}"
            }
    ];

    // Set chat trimming configuration
    AiAgentSummarizationByTokens summarization = new AiAgentSummarizationByTokens()
    {
        // Summarize old messages When the number of tokens stored in the conversation exceeds this limit
        MaxTokensBeforeSummarization = 32768,
        // Max number of tokens that the conversation is allowed to contain after summarization
        MaxTokensAfterSummarization = 1024
    };

    agent.ChatTrimming = new AiAgentChatTrimmingConfiguration(summarization);

    // Limit the number of times the LLM can request for tools in response to a single user prompt
    agent.MaxModelIterationsPerCall = 3;

    var createResult = await store.AI.CreateAgentAsync(agent, new Performer
    {
        suggestedReward = "your suggestions for a reward",
        employeeId = "the ID of the employee that made the largest profit",
        profit = "the profit the employee made"
    });

    // Set chat ID, prefix, agent parameters.
    // (specific country activates one query tool,"everywhere" activates another)
    var chat = store.AI.Conversation(
        createResult.Identifier,
        "Performers/",
        new AiConversationCreationOptions().AddParameter("country", "France"));

    // Handle the action tool that the LLM uses to store the performer's details in the database
    chat.Handle("store-performer-details", (Performer performer) =>
    {
        using (var session1 = store.OpenSession())
        {
            // Values provided by the LLM
            Performer rewarded = new Performer
            {
                suggestedReward = performer.suggestedReward,
                employeeId = performer.employeeId,
                profit = performer.profit
            };

            // store values in Performers collection in database
            session1.Store(rewarded);
            session1.SaveChanges();
        }
        return "done";
    });

    // Set user prompt and run chat
    chat.SetUserPrompt("send a few suggestions to reward the employee that made the largest profit");

    var LLMResponse = await chat.RunAsync<Performer>(CancellationToken.None);

    if (LLMResponse.Status == AiConversationResult.Done)
    {
        // The LLM successfully processed the user prompt and returned its response.
        // The performer's ID, profit, and suggested rewards were stored in the Performers
        // collection by the action tool, and are also returned in the final LLM response.
    }
}
```

<hr />

## Stream LLM responses

You can set the agent to [stream the LLM's response to the client](../../../ai-integration/ai-agents/ai-agents_overview#streaming-llm-responses) in real time as the LLM generates it, using the `StreamAsync` method, instead of using [RunAsync](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#setting-user-prompt-and-running-conversation) which sends the whole response to the client when it is fully prepared.  

Streaming the response allows the client to start processing it before it is complete, which can improve the application's responsiveness.  

* **Example**  
  ```csharp
  // A StringBuilder, used in this example to collect the streamed response
  var reward = new StringBuilder();

  // Using StreamAsync to collect the streamed response
  // The response property to stream is in this case `suggestedReward`
  var LLMResponse = await chat.StreamAsync<Performer>(responseObj => responseObj.suggestedReward, str =>
  {
    // Callback invoked with the arrival of each incoming chunk of the processed property

    reward.Append(str); // Add the incoming chunk to the StringBuilder instance
    return Task.CompletedTask; // Return with an indication that the chunk was processed

  }, CancellationToken.None);

  if (LLMResponse.Status == AiConversationResult.Done)
  {
    // Handle the full response when ready

    // The streamed property was fully loaded and handled by the callback above,
    // remaining parts of the response (including other properties if exist)
    // will arrive when the whole response is ready and can be handled here.
  }
  ```

* `StreamAsync` overloads:  

      ```csharp
      // The property to stream is indicated using a lambda expression
      Task<AiAnswer<TAnswer>> StreamAsync<TAnswer>
      (Expression<Func<TAnswer, string>> streamPropertyPath, 
      Func<string, Task> streamedChunksCallback, CancellationToken token = default);
      ```

      ```csharp
      // The property to stream is indicated as a string, using its name 
      Task<AiAnswer<TAnswer>> StreamAsync<TAnswer>
      (string streamPropertyPath, 
      Func<string, Task> streamedChunksCallback, CancellationToken token = default);
      ```

      | Property | Type | Description |
      |----------|------|-------------|
      | streamPropertyPath | `Expression<Func<TAnswer, string>>` | A lambda expression that selects the property to stream from the response object.<br /><ul><li>**Must be a simple string property**.</li><li>Strongly recommended that this would be the <strong>first property defined in the response schema</strong>.</li></ul> |
      | streamPropertyPath | `string` | The name of the property in the response object to stream.<br /><ul><li>**Must be a simple string property**.</li><li>Strongly recommended that this would be the <strong>first property defined in the response schema</strong>.</li></ul> |
      | streamedChunksCallback | `Func<string, Task>` | A callback function that is invoked with each incoming chunk of the streamed property |
      | token | `CancellationToken` | An optional token that can be used to cancel the streaming operation |

      | Return value | Description |
      |--------------|-------------|
      | `Task<AiAnswer<TAnswer>>` | Task result, including the LLM's final response and conversation status |



