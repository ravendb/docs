import Admonition from '@theme/Admonition';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';
import LanguageSwitcher from "@site/src/components/LanguageSwitcher";
import LanguageContent from "@site/src/components/LanguageContent";
import ContentFrame from "@site/src/components/ContentFrame";
import Panel from "@site/src/components/Panel";

<Admonition type="note" title="">

* To create an AI agent, the client defines its configuration, sets its parameters and tools, and registers the agent with the server.
    
* Once the agent is created, the client can initiate or resume **conversations**, get LLM responses, and perform actions based on LLM insights.
    
* This article provides a step-by-step guide to creating an AI agent and interacting with it using the **Client API**.  
  To create an AI agent from Studio, see [Creating AI agents - Studio](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_studio).
    
* In this article:
   * [Create a connection string](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#creating-a-connection-string)  
   * [Define the agent](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#defining-an-agent-configuration)
   * [Add agent tools](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#adding-agent-tools)
      * [Query tools](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#query-tools)
      * [Action tools](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#action-tools)
   * [Create the agent](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#creating-the-agent)
   * [Manage conversations](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#managing-conversations)
      * [Create a conversation](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#setting-a-conversation)
      * [Process action-tool requests](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#processing-action-tool-requests)
         * [Action-tool handlers](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#action-tool-handlers)
         * [Action-tool receivers](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#action-tool-receivers)
      * [Set user prompt and RUN the conversation](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#setting-user-prompt-and-running-the-conversation)
      * [Handle the conversation response](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#conversation-response)
      * [Stream LLM responses](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#stream-llm-responses)
   * [Full example](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#full-example)    
   * [Retrieve existing agents](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#retrieving-existing-agent-configurations)
   * [Delete agent](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#delete-agent)
   * [Syntax](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#syntax)

</Admonition>

<span id="creating-a-connection-string"></span>
<Panel heading="Create a connection string">

Your agent will need a connection string to connect to a **conversational or text generation LLM**.  
RavenDB supports the following providers for these model types:  
[Ollama](../../../ai-integration/connection-strings/ollama),
[OpenAI and compatible providers](../../../ai-integration/connection-strings/open-ai),
and [Azure OpenAI](../../../ai-integration/connection-strings/azure-open-ai).
   
Choose the model that best suits your needs:  
You can use a local _Ollama_ model if your priorities are speed, cost, open-source usage, or security.  
Or use a remote _OpenAI_ service for its broader resources and capabilities.
    
* **From the Client API**:  
  Create a connection string using an `AiConnectionString` instance and the `PutConnectionStringOperation` operation, as shown in the example below.    

* **From the Studio**:    
  You can define a connection string in the _AI Connection Strings_ view.
  See [AI connection strings - Overview](../../../ai-integration/connection-strings/overview).  
  You can also create a connection string when defining an AI agent.
  See [Configure basic settings](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_studio#configure-basic-settings).  

---
    
**Example**
    
<Tabs groupId='languageSyntax'>
<TabItem value="connection_string_to_ollama" label="connection_string_to_ollama">
```js
// Define the connection string to Ollama
const connectionString = new AiConnectionString();

// Connection string Name & Identifier
connectionString.name = "ConnectionStringToOllama";
connectionString.identifier = "identifier-to-the-connection-string"; // optional

// Model type
connectionString.modelType = "Chat";

// Ollama connection settings    
connectionString.ollamaSettings = new OllamaSettings(
    "http://localhost:11434", // Uri
    "llama3:8b-instruct");    // Name of chat model to use
    
// Deploy the connection string to the server    
const putConnectionStringOp = new PutConnectionStringOperation(connectionString);
const putConnectionStringResult = await documentStore.maintenance.send(putConnectionStringOp);
```
</TabItem>
<TabItem value="connection_string_to_open_ai" label="connection-string_to_open_ai">
```js
// Define the connection string to OpenAI
const connectionString = new AiConnectionString();

// Connection string Name & Identifier
connectionString.name = "ConnectionStringToOpenAI";
connectionString.identifier = "identifier-to-the-connection-string"; // optional
    
// Model type    
connectionString.modelType = "Chat";

// OpenAI connection settings
connectionString.openAiSettings = new OpenAiSettings(
    "your-api-key",
    "https://api.openai.com/v1",
    "gpt-4.1"); // Name of chat model to use  

// Deploy the connection string to the server    
const putConnectionStringOp = new PutConnectionStringOperation(connectionString);
const putConnectionStringResult = await documentStore.maintenance.send(putConnectionStringOp);
```
</TabItem>
<TabItem value="connection_string_to_azure_open_ai" label="connection-string_to_azure_open_ai">
```js
// Define the connection string to Azure OpenAI    
const connectionString = new AiConnectionString();

// Connection string Name & Identifier    
connectionString.name = "ConnectionStringToAzureOpenAI";
connectionString.identifier = "identifier-to-the-connection-string" // optional;
    
// Model type    
connectionString.modelType = "Chat";
    
// Azure OpenAI connection settings
connectionString.azureOpenAiSettings = new AzureOpenAiSettings(
    "your-api-key",
    "https://your-resource-name.openai.azure.com",
    "gpt-4o-mini", // Name of chat model to use
    "your-deployment-name");

const putConnectionStringOp = new PutConnectionStringOperation(connectionString);
const putConnectionStringResult = await documentStore.maintenance.send(putConnectionStringOp);
```
</TabItem>
</Tabs>

---
    
**Syntax reference**    
See the dedicated syntax sections in the following articles for full configuration details:    
* [Ollama (syntax)](../../../ai-integration/connection-strings/ollama#syntax)  
* [OpenAI and compatible providers (syntax)](../../../ai-integration/connection-strings/open-ai#syntax)  
* [Azure OpenAI (syntax)](../../../ai-integration/connection-strings/azure-open-ai#syntax)  

</Panel>

<span id="defining-an-agent-configuration"></span>
<Panel heading="Define the agent">

* To create an AI agent, start by creating an **agent configuration** object:
 
  <TabItem>    
  ```js
  const agentConfiguration = {};
  ```
  </TabItem>    
    
* Then populate the object with your system prompt, agent settings, and tools.  
  The following sections explain how to configure each component of the agent:  
  * [System prompt](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#system-prompt)  
  * [Agent name](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#agent-name)  
  * [Agent ID](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#agent-id)  
  * [Connection string](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#connection-string)  
  * [Expected response format](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#expected-response-format)  
  * [Agent parameters](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#agent-parameters)  
  * [Maximum number of iterations](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#maximum-number-of-iterations)  
  * [Chat trimming configuration](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#chat-trimming-configuration)      

---
    
<ContentFrame>    
### System prompt
    
This prompt defines the agent's role and capabilities.  
It provides general context to guide the LLM's responses throughout the conversation.

```js
agentConfiguration.systemPrompt = `
    You work for a human experience manager. 
    
    The manager uses your services to find which employee has made the largest profit
    and to suggest a reward. The manager provides you with the name of a country, 
    or with the word "everything" to indicate all countries.
    
    Then you:
    1. Use a query tool to load all the orders sent to the selected country,
       or a query tool to load all orders sent to all countries.
    2. Calculate which employee made the largest profit.
    3. Use a query tool to learn in what general area this employee lives.
    4. Find suitable vacation sites or other rewards based on the employee's residence area.
    5. Use an action tool to store in the database the employee's ID, profit,  
       and your reward suggestions.
       When you're done, return these details in your answer to the user as well.`;
```    
</ContentFrame>
    
<ContentFrame>
### Agent name
    
Set a unique name for the agent.

```js
agentConfiguration.name = "Reward productive employee";
```    
</ContentFrame>
    
<ContentFrame>
### Agent ID

Provide a unique identifier for the agent.  
Only lowercase letters (`a-z`), numbers (`0-9`) and hyphens (`-`) are allowed in the identifier.  
If not specified, it will be auto-generated from the agent name.    
 
```js
agentConfiguration.identifier = "reward-productive-employee";
``` 
</ContentFrame>
    
<ContentFrame>
### Connection string
    
Provide the name of the connection string you created above in [Create a connection string](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#creating-a-connection-string).    
    
```js
agentConfiguration.connectionStringName = connectionString.name;
```    
</ContentFrame> 
    
<ContentFrame>
### Expected response format   

Define a response format using a [structured output](https://platform.openai.com/docs/guides/structured-outputs) that the LLM will populate as its reply.  
This will be the format you expect to receive from the LLM via the agent during the conversation.      
You can define it in one of two ways:
    
* **Sample response object**:  
  Set the `sampleObject` property with a representative JSON object.  
  This object is not sent to the model directly - RavenDB uses it to generate a JSON schema, which is sent to the model.
  This option is simpler and suitable in most cases.
    
* **Custom JSON schema**:   
  Set the `outputSchema` property with a full JSON schema.  
  This gives you more control over the structure, types, and validation rules.
    
**Precedence rule**:  
If you define both `sampleObject` and `outputSchema`, only the schema will be sent to the model.
  
<Tabs groupId='sampleResponseObject'>
<TabItem value="Sample_response_object" label="Sample_response_object">
```js
// Sample response object
agentConfiguration.sampleObject = JSON.stringify({
        "suggestedReward": "Embed your suggestions for a reward here",
        "employeeId": "Embed the ID of the employee that made the largest profit here",
        "profit": "Embed the profit the employee made here"
    });
```    
</TabItem>
<TabItem value="JSON_schema" label="JSON_schema">
```csharp
// Response JSON schema
agent.OutputSchema = outputSchema: JSON.stringify({
    "name": "RHkxaWo5ZHhMM1RuVnIzZHhxZm9vM0c0UnYrL0JWbkhyRDVMd0tJa1g4Yz0",
    "strict": true,
    "schema": {
        "type": "array",
        "items": {
            "type": "object",
            "properties": {
                "employeeID": {
                    "type": "string",
                    "description": "the ID of the employee that made the largest profit"
                },
                "profit": {
                    "type": "string",
                    "description": "the profit the employee made"
                },
                "suggestedReward": {
                     "type": "string",
                     "description": "your suggestions for a reward"
                }
            },
            "required": [
                "employeeID",
                "profit",
                "suggestedReward"
            ],
            "additionalProperties": false
        }
    }
);
```
</TabItem>
</Tabs>    
</ContentFrame>
    
<ContentFrame>
### Agent parameters   

Agent parameters let you define named placeholders for values used in queries inside query tools.
    
At configuration time, you define the parameter name (e.g. `country`),  
which you can then use in the RQL of your [query tools](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#query-tools) as a placeholder (e.g. `$country`).  
The values for these parameters are Not set by the LLM - 
you must provide the actual value at [conversation startup](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#setting-a-conversation).

When the agent is requested to execute a query that references an agent parameter,   
it replaces each placeholder with the corresponding value you supplied at chat startup, before running the query.
    
```js
// Set agent parameters
agentConfiguration.parameters = [{
    name: "country", // Use '$country' in your query to reference this parameter
    description: `
        A specific country that orders were shipped to,
        or "everywhere" to look for orders shipped to all countries.`
}];
```
</ContentFrame>  
    
<ContentFrame>
### Maximum number of iterations
  
Set a limit on how many times the LLM is allowed to invoke agent tools in response to a single user prompt.  

<TabItem>
```js
agentConfiguration.maxModelIterationsPerCall = 3;
```
</TabItem>    
    
<Admonition type="note" title="">
* Note that you can reduce Time To First Byte (TTFB) by streaming the LLM response as it's being generated.  
  This allows the LLM to return selected fields in chunks before the full response is complete.  
* Find more about streaming in [Streaming LLM responses - overview](../../../ai-integration/ai-agents/overview#streaming-llm-responses) 
  and in [Stream LLM responses](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#stream-llm-responses) below.  
</Admonition>
    
</ContentFrame>
    
<ContentFrame>
### Chat trimming configuration 

You can configure RavenDB to automatically trim long conversations by summarizing older messages stored in the chat conversation document.
When the total number of tokens exceeds the configured threshold, RavenDB will generate a summary and replace the earlier part of the conversation with it.

Optionally, the original (unsummarized) conversation can be saved in a document under the `@conversations-history` collection.
You can also configure how long these history documents are retained before they expire.   
    
```js
// Set chat trimming configuration    
agentConfiguration.chatTrimming = {
    tokens: {
        // Summarization is triggered when the total number of tokens 
        // used in the conversation exceeds this limit.
        maxTokensBeforeSummarization: 32768,
        // The maximum number of tokens to retain in the conversation after summarization.
        // Older messages are removed.
        maxTokensAfterSummarization: 1024
    },
    history: {
        // Set how long conversation-history documents are retained (in seconds)    
        historyExpirationInSec: 86400, // 1 day
    }
};
```    
</ContentFrame>
    
</Panel>

<Panel heading="Adding agent tools">
    
* You can enhance your agent with **Query tools** and **Action tools**,  
  components that allow the LLM to query your database and trigger client-side actions.  

* Once tools are defined and submitted as part of the agent configuration,  
  it’s up to the LLM to decide **if** and **when** to invoke them during a conversation.

---
    
<ContentFrame>

### Query tools

* [Query tools](../../../ai-integration/ai-agents/overview#query-tools) provide the LLM with the ability to retrieve data from the database.  
  Each query tool includes:  
  * **Description** - a natural-language description that tells the LLM when to use it, 
  * **RQL** - an [RQL query](../../../client-api/session/querying/what-is-rql) that defines what data to retrieve.    
    
* To run a query tool at agent startup and provide initial context to the LLM **before** the conversation begins,  
  see: [Initial-context queries](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#initial-context-queries) below.

* **Passing values to a query tool**  
  The RQL in the query tool may include parameter placeholders prefixed with `$` (e.g. `$country`).  
  Both the user and the LLM can pass values to these parameters.  
  * **Passing values from the user**:  
    Users can pass values to queries through [Agent parameters](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#agent-parameters).  
    If agent parameters are defined in the agent configuration -  
    * The client must provide values for them when initiating a conversation with the agent.
    * Before executing the query, the agent will replace the placeholders with the provided values.
  * **Passing values from the LLM**:  
    You can define parameters that the LLM will fill in dynamically based on the conversation context when it invokes the query.
    To do this, define a sample object (or a formal JSON schema) that describes the parameters the LLM is expected to supply when requesting the agent to run the query.    
    * If both a sample object and a JSON schema are defined, the schema is used.  
    * If only a sample object is provided, RavenDB will convert it into a JSON schema.  
    * When the LLM triggers the tool, it will fill in values for the defined parameters based on the conversation.
  * Note:  
    You cannot define both an agent parameter and a tool parameter with the same name.  
    Each parameter name must be unique across both types.

* **Example**  
  The example below defines three query tools:  
  * **The first query tool** is used by the LLM to retrieve all orders sent anywhere in the world.  
    The system prompt instructs it to use this tool when the user starts the conversation with the value "everywhere".
  * **The second query tool** retrieves all orders sent to a specific country,  
    using the `$country` agent parameter provided by the client at conversation startup.
  * **The third query tool** retrieves the general location of an employee,  
    using the `$employeeId` parameter, whose value is set by the LLM when it requests to run this tool.  

        ```js
        agentConfiguration.queries = [
            {
                // Set a query tool to retrieve all orders sent everywhere.
                // Query tool name
                name: "retrieve-orders-sent-to-all-countries",
    
                // Query description
                description:
                    `A query that allows you to retrieve all orders sent to all countries`,
    
                // RQL query
                query:
                    `from Orders as o
                    select o.Employee, o.Lines.Quantity`,
    
                // Sample parameters object for the query tool, here no params are defined
                parametersSampleObject: JSON.stringify({})
            },
            {
                // Set a query tool to retrieve all orders sent to a specific country.
                // The country is provided by the user as an agent parameter.
                name: "retrieve-orders-sent-to-a-specific-country",
                description:
                    `A query that allows you to retrieve all orders sent to a specific country`,
                query:
                    `from Orders as o 
                    where o.ShipTo.Country == $country
                    select o.Employee, o.Lines.Quantity`,
                parametersSampleObject: JSON.stringify({})
            },
            {
                // Set a query to retrieve the performer's residence details from the database.
                // The employee ID is provided by the LLM when it requests to run this tool.
                name: "retrieve-performer-living-region",
                description:
                   `A query that allows you to retrieve an employee's country,
                    city, and region, by the employee's ID`,
                query:
                    `from Employees as e
                    where id() == $employeeId
                    select e.Address.Country, e.Address.City, e.Address.Region`,
                parametersSampleObject: JSON.stringify({
                    "employeeId": "Embed the employee's ID here"
                })
            }
        ];
        ```

---
    
#### <u>Initial-context queries</u>

* Use the `options.addToInitialContext` property to configure a query tool as an [initial-context query](../../../ai-integration/ai-agents/overview#initial-context-queries)
  so that it executes immediately when the agent starts, before the LLM receives any user input.  
  The results are provided to the LLM as part of the initial conversation context.
 
   * An initial-context query is **not allowed** to use LLM parameters because the LLM has no opportunity to supply values - the query runs before the conversation starts.
   * An initial-context query **can use** agent parameters, since their values are supplied by the client at conversation startup.

*  Use the `options.allowModelQueries` property to control whether the LLM is allowed to trigger the query tool later in the conversation.
   * If `allowModelQueries` is _true_, the LLM can trigger the query anytime during the conversation.
   * If `allowModelQueries` is _false_, the LLM cannot invoke the query tool.
   * If the query tool is set as an initial-context query, it will be executed at startup regardless of the `allowModelQueries` setting. 

* **Example**  
  Set a query tool to retrieve all orders sent worldwide.  
  The query will run when the agent is started.   
    
  ```js
  agentConfiguration.queries = [
      {
          name: "retrieve-orders-sent-to-all-countries",
          description:
              `A query that allows you to retrieve all orders sent to all countries`,
          query:
              `from Orders as o
              select o.Employee, o.Lines.Quantity`,
          parametersSampleObject: JSON.stringify({}), 
    
          // Initial-context query configuration
          options: {

              // Run the query at conversation startup and provide its results to the LLM
              addToInitialContext: true,
                  
              // Also allow the LLM to trigger this query later in the conversation
              allowModelQueries: true
          }
      }
  ];
  ```
</ContentFrame>
    
<ContentFrame>

### Action tools

* Action tools allow the LLM to instruct the client to perform an operation (e.g., to modify or create a document).  
  This communication is mediated by the agent, which receives the tool call from the LLM and passes the request to the client.
    
    Each action tool includes:  
    * **Description** - a natural-language description that tells the LLM what the tool does,  
    * **Schema** - a schema that the LLM will fill with the required action data before sending it to the agent.
    
* Once the client completes the requested action, it must send a response back to the LLM indicating the result,
  for example, `"done"`. 

* In the example below, the action tool requests the client to store an employee’s details in the database.  
  The LLM will provide the employee's ID and other details whenever it triggers the tool.

  ```js
  agentConfiguration.actions = [
      {
          // Set an action tool to store the performer's details
          name: "store-performer-details",
          description: 
              `An action tool that allows you to store the ID of the employee that made the
               largest profit, the profit amount, and your reward suggestion in the database.`,
          parametersSampleObject: JSON.stringify({
              "suggestedReward": "Embed your suggestions for a reward here",
              "employeeId": "Embed the employee’s ID here",
              "profit": "Embed the employee’s profit here"
          })
      }
  ];
  ```
    
</ContentFrame>

</Panel>

<span id="creating-the-agent"></span>
<Panel heading="Create the agent">

* Once the agent configuration is complete,  
  register the agent with the server using the `createAgent` method:
  * Define a response object class that matches the response schema in your agent configuration.    
  * Call `createAgent` and pass:  
    * The agent configuration  
    * A new instance of the response object class  

* **Example**  
    
  <Tabs groupId='languageSyntax'>  
  <TabItem value="Create_agent" label="Create_agent">
  ```js
  const createdAgentResult = await documentStore.ai.createAgent(
      agentConfiguration,
      new Performer(
          "Your suggestions for a reward", 
          "The ID of the employee that made the largest profit",
          "The profit the employee made"));
  ```
  </TabItem>
  <TabItem value="Performer_class" label="Performer_class">
  ```js
  class Performer {
      constructor(suggestedReward, employeeId, profit) {
          this.suggestedReward = suggestedReward;
          this.employeeId = employeeId;
          this.profit = profit;
      }
  }
  ```
  </TabItem>
  </Tabs>
    
</Panel>

<span id="managing-conversations"></span>
<Panel heading="Manage conversations">

<ContentFrame>

<span id="setting-a-conversation"></span>
### Create a conversation:

* Create a conversation using the `documentStore.ai.conversation` method. Pass:  
  * The agent ID.  
  * The conversation ID or conversation document prefix.
  * Conversation creation options - including values for any agent parameters, if defined.

* The object returned by `conversation` is used to run the chat.  
   See [Set user prompt and run the conversation](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#setting-user-prompt-and-running-the-conversation).

* **Example**  
  <TabItem>    
  ```js
  const chat = documentStore.ai.conversation(
      createdAgentResult.identifier, // The agent ID 
      "Performers/",                 // The conversation document prefix
      {
          parameters: {
              country: "France"      // The agent parameter
          }
      });
  ```
  </TabItem>

  <Admonition type="note" title="">
      
  Conversations are stored as documents in the `@conversations` collection.  
  The conversation ID or prefix you provide determines whether a new conversation will start or an existing one will resume:
      
  * **Start a new conversation**  
    To start a new conversation, provide a prefix ending with  `/` or `|` (e.g., `Performers/`).  
    RavenDB will auto-generate the rest of the conversation document ID (see [document ID generation](../../../server/kb/document-identifier-generation)).
      
  * **Resume an existing conversation**  
    To resume an existing conversation, provide the full ID of an existing conversation document  
    (e.g., `Performers/0000000000000008883-A`).  
    The conversation will be retrieved from storage and resumed from where it left off.
      
  </Admonition>
    
</ContentFrame>
    
<ContentFrame>

<span id="processing-action-tool-requests"></span>
### Process action-tool requests

* During the conversation, the LLM may request the agent to trigger an action tool.  
  When this happens, the agent forwards the tool’s name and parameters to the client -  
  and it’s up to the client to handle the request.

* The client can process an action-tool request using either a [Handler](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#action-tool-handlers) 
  or a [Receiver](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#action-tool-receivers).  

---

#### <u>Action-tool Handlers</u>
    
A **handler** is created for a specific action tool and registered with the server using the `handle` method.  
When the LLM triggers the action tool, the handler is invoked to process the request.
After processing is complete, the handler returns a result to the agent, which then sends it to the LLM as the tool’s response.    

<Admonition type="note" title="">
**When to use a handler**:      
Handlers are typically used for simple, immediate operations such as storing a document in the database and returning a confirmation, 
performing a quick calculation and returning the result, or any scenario where the response can be generated and returned in a single step.  
</Admonition>

* To **create a handler**, call the `handle` method and pass:  
  * The action tool's name.  
  * A callback function that receives an object matching the structure of the action tool’s parameters schema.  
    This object will be automatically populated with data from the LLM when it triggers the tool.    

* When you **finish processing the requested action**, simply `return` a result.  
  The agent will forward this result to the LLM as the tool’s response.

* **Example**  
  This handler stores the performer’s details in the database when the LLM triggers the `store-performer-details` action tool.  

  <Tabs groupId='languageSyntax'>  
  <TabItem value="Handler" label="Handler">
  ```js
  // Handler for the "store-performer-details" action tool
  chat.handle(
      "store-performer-details", // Action tool's name
      async performer => {       // Handler callback  
          const session = documentStore.openSession();
  
          // Store the performer’s data in the database    
          await session.store(performer);
          await session.saveChanges();
      
          // Return a response back to the LLM   
          return "done";
      }
  );     
  ```
  </TabItem>
  <TabItem value="Performer_class" label="Performer_class">
  ```js
  // Class matching the action tool’s parameters schema         
  class Performer {
      constructor(suggestedReward, employeeId, profit) {
          this.suggestedReward = suggestedReward;
          this.employeeId = employeeId;
          this.profit = profit;
      }
  }
  ```
  </TabItem>
  </Tabs>
    
---

#### <u>Action-tool Receivers</u>
                    
A **receiver** is created for a specific action tool and registered with the server using the `receive` method.    
When the LLM triggers the action tool, the receiver is invoked to process the request.    
Unlike a _handler_, the receiver remains active until `addActionResponse` is explicitly called to complete the pending request and send a response to the LLM.    
  
<Admonition type="note" title="">
**When to use a receiver**:   
Receivers are typically used for asynchronous, multi-step, or delayed operations, such as waiting for user input or an external event,
performing a long-running task like batch processing or external system integration, or any case where the response cannot be generated immediately.   
</Admonition>

* To **create a receiver**, call the `receive` method and pass:   
  * The action tool's name.  
  * A callback function that receives:
    * A request object containing metadata about the request (e.g., the tool ID).
    * An object that matches the structure of the action tool’s parameters schema.  
      This object will be automatically populated with data from the LLM when it triggers the tool. 

* When you **finish handling the requested action**, call `addActionResponse` and pass:  
   * The action tool's ID.  
   * The response to send back to the LLM.      
     <Admonition type="note" title="">
     Note that the response can be sent at any time, even after the receiver finishes executing,  
     and from any context, not just inside the receiver callback.  
     </Admonition>

* **Example**  
  In this example, the receiver stores the performer’s details, sends a notification, and then responds to the LLM.
    
  <Tabs groupId='languageSyntax'>  
  <TabItem value="Receiver" label="Receiver">
  ```js
  // Receiver for the "store-performer-details" action tool
  chat.receive(
      "store-performer-details",       // Action tool's name
      async (request, performer) => {  // Receiver callback      
          const session = documentStore.openSession();
      
          // Store performer details
          await session.store(performer);
          await session.saveChanges();
  
          // Perform a long-running operation
          // For example, send a notification email
          // (emailService is assumed to be defined elsewhere)
          await emailService.SendNotification("manager@company.com", performer);
  
          // Call 'addActionResponse' to send a response back to the LLM when done
          // and close the request
          chat.addActionResponse(request.toolId, "done");
      }
  );
  ```    
  </TabItem>
  <TabItem value="Performer_class" label="Performer_class">
  ```js
  // Class matching the action tool’s parameters schema         
  class Performer {
      constructor(suggestedReward, employeeId, profit) {
          this.suggestedReward = suggestedReward;
          this.employeeId = employeeId;
          this.profit = profit;
      }
  }
  ```
  </TabItem>
  </Tabs>

</ContentFrame>
    
<ContentFrame>

<span id="setting-user-prompt-and-running-the-conversation"></span>     
### Set user prompt and RUN the conversation

Set the user prompt using the `setUserPrompt` method, and run the conversation using the `run` method.

<TabItem>    
```js
// Set the user prompt:
chat.setUserPrompt(
    `Send suggestions to reward the employee that made the largest profit
    and store the results in the database`);

// Run the conversation:
const llmResponse = await chat.run();

// Check the LLM's response status    
if (llmResponse.status === "Done") {
    // The LLM successfully processed the user prompt and returned a response.
    // The performer's ID, profit, and suggested rewards were stored in the Performers
    // collection by the action tool, and are also included in the final LLM response.
    const answer = llmResponse.answer;
}
```
</TabItem>

<Admonition type="note" title="">
Instead of `run`, you can use `stream` to **stream** the LLM's response as it is generated.  
See [Stream LLM responses](../../../ai-integration/ai-agents/creating-ai-agents/creating-ai-agents_api#stream-llm-responses).
</Admonition>    

</ContentFrame>
    
<ContentFrame>

<span id="conversation-response"></span>
### Handle the conversation response:

Each time you call `run()`, the agent returns a **response object** to the client.  
This object contains:  
* `answer` -  The LLM's reply to the user prompt (if available).  
* `status` - The current state of the conversation.  
* `usage` - Token usage reported by the model for generating this answer.  
            Reflects usage for the current turn only.  
* `elapsed` - The total time elapsed to produce the answer.  
              Measured from the server's request to the LLM until the response was received.

The status can be:
* `"Done"`  
  The conversation is complete, and a final answer is available in the answer field.  
* `"ActionRequired"`  
  The conversation requires further interaction.
  For example, the LLM may have triggered a tool request, and the conversation is paused until the client processes it.

</ContentFrame>
    
<ContentFrame>
    
### Stream LLM responses

* Instead of calling `run`, which returns the LLM's response to the client when it is fully prepared,  
  you can call `stream` to [Stream llm responses](../../../ai-integration/ai-agents/overview#streaming-llm-responses),
  and receive the LLM's response in real time as it is being generated.

* Streaming allows the client to start processing the response before it is complete,  
  which can improve the application's responsiveness and perceived speed.    
    
* The selected property to stream must be a simple `string` (and not a JSON object or an array, for example).
    
* It is recommended that the property to stream would be the first one defined in the response schema.  
  The LLM processes the properties in the order they are defined. 
  Streaming the first property will ensure that streaming starts immediately even if it takes the LLM time to process later properties.

* **Example**
  ```js
  let rewardText = "";
    
  // Call 'stream' to collect the streamed response
  const streamedAnswer = await chat.stream(
    
      // The response property to stream
      "suggestedReward",
    
      // Callback function invoked with each incoming chunk of the streamed property    
      async (chunk) => {  
          console.log("stream chunk", chunk);
          rewardText += chunk;
  });
  
  // Check the conversation status
  if (llmResponse.status === "Done") {
  {    
      console.log("Final streamed answer", streamedAnswer);
    
      // The streamed property (`suggestedReward`) was processed chunk by chunk above
      // and is fully received.
      // Other properties in the response (e.g., employeeId, profit) are not streamed,
      // they will be available in the final response object once the conversation is complete.
  }
  ```
</ContentFrame>
    
</Panel>

<Panel heading="Full example">
 
In this example, the agent’s user is a **Human Experience Manager**.  
The agent assists the user in rewarding top-performing employees by following these steps:
    
* **Search for relevant orders**:  
  The agent uses a **Query Tool** to retrieve orders shipped to a specific country,  
  or to all countries if the user prompts it with "everywhere".
* **Identify the top performer**:  
  From the retrieved orders, it calculates which employee generated the highest profit.
* **Retrieve employee details**:  
  Using the employee’s ID from the top order,  
  the agent runs another **Query Tool** to fetch the employee’s region of residence.
* **Find suitable rewards**:  
  Based on the employee’s region, the agent looks up appropriate reward options.
* **Store and respond**:  
  It uses an **Action Tool** to store the employee’s ID, profit, and suggested rewards in the `Performers` collection.  
  The same information is also returned in the agent’s final response.

```js
// Define a connection string to OpenAI
// ====================================

const connectionString = new AiConnectionString();

// Connection string Name & Identifier
connectionString.name = "ConnectionStringToOpenAI";
connectionString.identifier = "identifier-to-the-connection-string"; // optional
    
// Model type    
connectionString.modelType = "Chat";

// OpenAI connection settings
connectionString.openAiSettings = new OpenAiSettings(
    "your-api-key",
    "https://api.openai.com/v1",
    "gpt-4.1"); // Name of chat model to use  

// Deploy the connection string to the server    
const putConnectionStringOp = new PutConnectionStringOperation(connectionString);
const putConnectionStringResult = await documentStore.maintenance.send(putConnectionStringOp);

// DEFINE THE AGENT
// ================

const agentConfiguration = {
    name: "Reward productive employee",
    connectionStringName: connectionString.name,
    systemPrompt: `
        You work for a human experience manager.
    
        The manager uses your services to find which employee has made the largest profit
        and to suggest a reward. The manager provides you with the name of a country,
        or with the word "everything" to indicate all countries.
    
        Then you:
        1. Use a query tool to load all the orders sent to the selected country,
           or a query tool to load all orders sent to all countries.
        2. Calculate which employee made the largest profit.
        3. Use a query tool to learn in what general area this employee lives.
        4. Find suitable vacation sites or other rewards based on the employee's residence area.
        5. Use an action tool to store in the database the employee's ID, profit, 
           and your reward suggestions.
           When you're done, return these details in your answer to the user as well.`,
    
    // Optionally, set the agent ID 
    // If not provided, the identifier will be auto-generated from the agent's name.
    identifier: "reward-productive-employee",

    // Define the LLM response object
    sampleObject: JSON.stringify({
        "suggestedReward": "Embed your suggestions for a reward here",
        "employeeId": "Embed the ID of the employee that made the largest profit here",
        "profit": "Embed the profit the employee made here"
    }),

    // Set agent parameters
    parameters: [{
        name: "country",
        description: `A specific country that orders were shipped to,
            or "everywhere" to look for orders shipped to all countries.`
    }],

    // Set a limit on how many times the LLM is allowed to invoke agent tools
    // in response to a single user prompt.  
    maxModelIterationsPerCall: 3,

    // Set chat trimming configuration
    chatTrimming: {
        tokens: {
            // Summarize old messages when the number of tokens in the conversation
            // exceeds this limit:
            maxTokensBeforeSummarization: 32768,
            // Max number of tokens that the conversation is allowed to contain after summarization:
            maxTokensAfterSummarization: 1024
        }
    },

    // ADD AGENT TOOLS
    // ===============

    // Query tools:
    queries: [
        {
            // Set a query tool to retrieve all orders sent everywhere
            name: "retrieve-orders-sent-to-all-countries",
            description: "A query that allows you to retrieve all orders sent to all countries",
            query: `from Orders as o 
                select o.Employee, o.Lines.Quantity`,
            parametersSampleObject: JSON.stringify({}),
        },
        {
            // Set a query tool to retrieve all orders sent to a specific country
            name: "retrieve-orders-sent-to-a-specific-country",
            description: "A query that allows you to retrieve all orders sent to a specific country",
            query: `from Orders as o 
                where o.ShipTo.Country == $country
                select o.Employee, o.Lines.Quantity`,
            parametersSampleObject: JSON.stringify({})
        },
        {
            // Set a query tool to retrieve the performer's residence details from the database
            name: "retrieve-performer-living-region",
            description: `A query that allows you to retrieve an employee's country,
                city, and region, by the employee's ID`,
            query: `from Employees as e
                where id() == $employeeId
                select e.Address.Country, e.Address.City, e.Address.Region`,
            parametersSampleObject: JSON.stringify({
                "employeeId": "Embed the employee's ID here"
            })
        }
    ],

    // Action tools:
    actions: [
        {
            // Set an action tool to store the performer's details
            name: "store-performer-details",
            description:
               `An action tool that allows you to store the ID of the employee that made the
               largest profit, the profit amount, and your reward suggestion in the database.`,
            parametersSampleObject: JSON.stringify({
                "suggestedReward": "Embed your suggestions for a reward here",
                "employeeId": "Embed the employee’s ID here",
                "profit": "Embed the employee’s profit here"
            })
        }
    ]
}

// Create/deploy the agent
// =======================
const createdAgentResult = await documentStore.ai.createAgent(agentConfiguration,
    new Performer("Your suggestions for a reward", 
        "The ID of the employee that made the largest profit",
        "The profit the employee made"));

// Create a conversation/chat with the agent
// =========================================
const chat = documentStore.ai.conversation(
     createdAgentResult.identifier, // The agent ID 
     "Performers/",                 // The conversation document prefix
     {
        parameters: {
            country: "France"       // The agent parameter
     }
});

// Define a handler for the "store-performer-details" action tool
// ==============================================================
chat.handle("store-performer-details", async performer => {
    const session = documentStore.openSession();

    // Store the performer details in the database
    await session.store(performer);
    await session.saveChanges();

    return "done";
}) 

// Set user prompt:
// ================
chat.setUserPrompt(`
    Send suggestions to reward the employee that made the largest profit
    and store the results in the database.
`);

// Run the chat/conversation:
// ==========================
const llmResponse = await chat.run()

if (llmResponse.status === "Done") {
    // The LLM successfully processed the user prompt and returned a response.
    // The performer's ID, profit, and suggested rewards were stored in the Performers
    // collection by the action tool, and are also included in the final LLM response.
    const answer = llmResponse.answer;
}
```
</Panel>

<span id="retrieving-existing-agent-configurations"></span>
<Panel heading="Retrieve existing agents"> 

You can retrieve the configuration of **an existing agent** using `getAgent`.  
  
```js
// Retrieve an existing agent configuration by its ID
const existingAgent = await documentStore.ai.getAgent("reward-productive-employee");
```

You can also retrieve the configurations of **all existing agents** using `getAgents`.  

```js
// Retrieve ALL existing agentS
const existingAgentsList = await documentStore.ai.getAgents();
const agents = existingAgentsList.aiAgents;
```

</Panel>

<Panel heading="Delete agent"> 

You can delete the configuration of **an existing agent** using `deleteAgent`.  
  
```js
// Delete an existing agent configuration by its ID
const result = await documentStore.ai.deleteAgent("reward-productive-employee");
```

</Panel>

<Panel heading="Syntax">
    
### Agent configuration
    
`AiAgentConfiguration`
    
<TabItem>
```js
// The agent configuration object
{
    // A unique identifier given to the AI agent.
    identifier, // string
    
    // The agent name.
    name, // string
    
    // The name of the connection string used to connect to the LLM service. 
    connectionStringName, // string
    
    // The system prompt that defines the role and purpose of the agent and the LLM.
    systemPrompt, // string
    
    // An example object (as string) that sets the expected format of the LLM's response.
    // The object is translated to a schema before it is sent to the LLM.
    sampleObject, // string
    
    // A JSON schema that sets the expected format of LLM's response.
    // If both a sample object and a schema are defined, only the schema is used.
    outputSchema, // string
    
    // A list of Query tools that the LLM can use (through the agent) to access the database.
    // The LLM decides when to call them based on user input and context.
    queries, // AiAgentToolQuery[]
    
    // A list of Action tools that the LLM can use to trigger the user to action.
    // The LLM decides when to call them based on user input and context.
    actions, // AiAgentToolAction[]
    
    // The agent parameters used in the query tools.
    // Their values must be provided each time you start a new chat.   
    parameters, // AiAgentParameter[]
    
    // Define if and how the conversation is summarized, 
    // to minimize the amount of data passed to the LLM when a conversation is started. 
    chatTrimming, // AiAgentChatTrimmingConfiguration
    
    // The maximum number of times the LLM is allowed to invoke agent tools
    // in response to a single user prompt.  
    maxModelIterationsPerCall, // number
    
    // Indicate whether the agent is disabled.
    disabled // boolean
}
```    
</TabItem> 
    
`AiAgentToolQuery`
    
<TabItem>
```js
// The agent query tool object
{
    // The name of the query tool.
    name, // string
    
    // A description of the query tool.
    // This helps the LLM understand when to invoke this query.
    description, // string
    
    // The RQL query that will be executed against the database when this query tool is invoked.   
    query, // string
    
    // A sample object representing the query parameters 
    // that the LLM is expected to provide when invoking this query tool.
    parametersSampleObject, // JSON-formatted string
    
    // The JSON schema representing the query parameters.    
    // If both a sample object and a schema are defined, only the schema is used.
    parametersSchema, // string
    
    // Options for the query tool.
    options // AiAgentToolQueryOptions
}
```    
</TabItem>    
    
`AiAgentToolQueryOptions`
    
<TabItem>
```js
// The query tool options
{
    // true: the model is allowed to execute this query on demand based on its own judgment.
    // false: the model cannot call this query (unless executed as part of initial context).
    // null: server-side defaults apply
    allowModelQueries,  // boolean
    
    // true: the query is executed when conversation starts 
    //       and its results are added to the initial context.
    // false: the query is not be executed for the initial context.
    // null: server-side defaults apply
    addToInitialContext // boolean
}
```    
</TabItem> 

`AiAgentToolAction`
    
<TabItem>
```js
// The agent action tool object
{
    // The name of the action tool.
    name, // string
    
    // A description of the action tool.    
    // This helps the LLM understand when to trigger this action.
    description, // string
    
    // Define the format in which the LLM will supply data for the requested action
    // when it decides to trigger this action tool.
    // The LLM will fill in values for the specified fields based on the conversation context
    // and any relevant data it has access to.
    parametersSampleObject, // JSON-formatted string
    
    // The JSON schema defines the structure and types of the output you expect from the model.
    // If both a sample object and a schema are defined, only the schema is used.
    parametersSchema // string
}
```    
</TabItem>
    
`AiAgentParameter`
    
<TabItem>
```js
// The agent parameter object
{
    // The name of the parameter.
    name, // string
    
    // A description of the parameter.
    description, // string
    
    // Controls whether the parameter value should be sent to the LLM.
    // `false`: The parameter is hidden from the model (not included in prompts/echo messages).
    //          Use this for sensitive values like userId, tenantId, companyId, etc.
    // `true`: The parameter is explicitly exposed to the model.
    // `undefined` (default): Treated as exposed to the model.
    sendToModel // boolean
}
```    
</TabItem>

`AiAgentChatTrimmingConfiguration`
    
<TabItem>
```js
// The chat trimming object
{
    // Options for trimming the chat messages into a compact prompt
    // when token count exceeds a threshold.
    tokens, // AiAgentSummarizationByTokens
    
    // History documents are the copies of chat messages that have been summarized or truncated.
    // if null, no conversation history documents are created when conversation trimming occurs.
    history // AiAgentHistoryConfiguration
}
```    
</TabItem>
    
`AiAgentSummarizationByTokens`
    
<TabItem>
```js
// The chat summarization configuration object
{
    // Summarization will be triggered when the total number of tokens used in the conversation
    // exceeds this limit.
    maxTokensBeforeSummarization, // number
    
    // The maximum number of tokens to retain in the conversation after summarization.
    // Messages exceeding this limit will be removed, starting from the oldest.
    // Default: 1024
    maxTokensAfterSummarization // number
}
```    
</TabItem>
    
`AiAgentHistoryConfiguration`
    
<TabItem>
```js
// The conversation history configuration object
{
   
   // This property defines the timespan after which conversation history documents expire. 
   historyExpirationInSec // number
}
```    
</TabItem>
    
---    
    
### Creating the agent

<TabItem>
```js    
// Create or update an AI agent configuration
await documentStore.ai.createAgent(configuration, sampleObject)    
```    
</TabItem>
    
| Parameter         | Type     | Description              |
|-------------------|----------|--------------------------|
| **configuration** | `object` | The agent configuration. |
| **sampleObject**  | `object` | Example response object. |
    
<TabItem>
```js    
// Return value of createAgent
{
    identifier, //The agent ID, string
    raftCommandIndex // number
}
```    
</TabItem>
    
---    
    
### Create a conversation

<TabItem>
```js    
// Open a conversation with an agent.
documentStore.ai.conversation(agentId, conversationId, creationOptions, changeVector) 
```    
</TabItem>
    
| Parameter           | Type     | Description          |
|---------------------|----------|----------------------|
| **agentId**         | `string` | The agent unique ID. |
| **conversationId**  | `string` | The conversation document ID or a conversation document prefix (to auto-generate the ID). |
| **creationOptions** | `object` | Conversation creation options. |
| **changeVector**    | `string` | An optional change vector for concurrency control. |
    
`AiConversationCreationOptions`
    
<TabItem>
```js
// The conversation creation options object
{
    // Values for agent parameters defined in the agent configuration.
    parameters, // Record<string, unknown>
    
    // Optional expiration time (in seconds).
    // If the conversation is idle for longer than this, it will be automatically deleted.
    expirationInSec // number
}
```    
</TabItem>
    
<TabItem>
```js    
// Return value of creating a conversation
// =======================================

// The operations in this object allow you to set user prompts, run the conversation,  
// add code to handle action tools, and send responses.
    
class AiConversation {    
    // Set the user prompt for the conversation
    setUserPrompt(userPrompt);
    
    // Define a handler to handle an action tool, 
    // the handler returns the action response back to the LLM directly.    
    handle(actionName, action, aiHandleError);
    
    // Define a receiver to handle an action tool, 
    // Need to explicitly call 'addActionResponse' to send the action response back to the LLM.
    receive(actionName, action, aiHandleError);
    
    // Closes the action request and sends the response back to the LLM.
    addActionResponse(toolId, actionResponse);
    
    // Retrieve the list of action-tool requests the AI agent needs you to execute.
    requiredActions(): AiAgentActionRequest[];
    
    // Execute one “turn” of the conversation: 
    // Sends the current prompt, processes any required actions, and awaits the agent’s reply.
    run();
    
    // Execute one “turn” of the conversation streaming the specified property's value 
    // for immediate feedback.  
    // Sends the current prompt, processes any required actions, 
    // and awaits the agent’s reply while invoking the callback with streamed values.
    stream(streamPropertyPath, streamCallback);
}
```    
</TabItem>
    
| Parameter              | Type     | Description          |
|------------------------|----------|----------------------|
| **userPrompt**         | `string` | The text of the user’s message. |
| **actionName**         | `string` | The name of the action tool to handle. |
| **action**             | `(args) => object` | A callback function that processes the arguments and returns a response to the LLM. |
| **aiHandleError**      | `string` | An optional strategy for handling errors during execution.<br />`SendErrorsToModel` - Send errors to the model for handling.<br />`RaiseImmediately` - throw error exceptions. |    
| **toolId**             | `string` | The identifier of the action request. |
| **actionResponse**     | `string` | The response to send back to the LLM through the agent. |
| **streamPropertyPath** | `string` | The property in the response object to stream.<br />Must be a simple string. |
| **streamCallback**     | `(chunk) => void` | This callback is invoked for each incoming streamed chunk from the LLM response. |
    
</Panel>