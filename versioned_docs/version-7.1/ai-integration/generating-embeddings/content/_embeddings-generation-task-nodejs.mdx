import Admonition from '@theme/Admonition';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';
import ContentFrame from '@site/src/components/ContentFrame';
import Panel from '@site/src/components/Panel';

<Admonition type="note" title="">

* In RavenDB, you can define AI tasks to automatically generate embeddings from your document content.  
  These embeddings are then stored in [dedicated collections](../../../ai-integration/generating-embeddings/embedding-collections.mdx) within the database,  
  enabling [Vector search](../../../ai-integration/vector-search/overview.mdx) on your documents.

* This article explains how to configure such a task.  
  It is recommended to first refer to this [Overview](../../../ai-integration/generating-embeddings/overview.mdx#embeddings-generation---overview)
  to understand the embeddings generation process flow.

* In this article:
    * [Configuring an embeddings generation task - from the Studio](../../../ai-integration/generating-embeddings/embeddings-generation-task.mdx#configuring-an-embeddings-generation-task-from-the-studio)
    * [Configuring an embeddings generation task - from the Client API](../../../ai-integration/generating-embeddings/embeddings-generation-task.mdx#configuring-an-embeddings-generation-task-from-the-client-api)
      * [Define source using PATHS](../../../ai-integration/generating-embeddings/embeddings-generation-task.mdx#configure-an-embeddings-generation-task---define-source-using-paths)
      * [Define source using SCRIPT](../../../ai-integration/generating-embeddings/embeddings-generation-task.mdx#configure-an-embeddings-generation-task---define-source-using-script)
    * [Chunking methods and tokens](../../../ai-integration/generating-embeddings/embeddings-generation-task.mdx#chunking-methods-and-tokens)
    * [Syntax](../../../ai-integration/generating-embeddings/embeddings-generation-task.mdx#syntax)
    
</Admonition>

<Panel heading="Configuring an embeddings generation task - from the Studio">

* **Define the general task settings**:

     ![Create embeddings generation task - general](../assets/add-ai-task-3.png)

  1. **Name**  
     Enter a name for the task.
  2. **Identifier**  
     Enter a unique identifier for the task.  
     Each AI task in the database must have a distinct identifier.  

        If not specified, or when clicking the "Regenerate" button,  
        RavenDB automatically generates the identifier based on the task name. For example:     
        * If the task name is: _"Generate embeddings from OpenAI"_
        * The generated identifier will be: _"generate-embeddings-from-openai"_
      
        Allowed characters: only lowercase letters (a-z), numbers (0-9), and hyphens (-).  
  
        **This identifier is used:**  
        * When querying embeddings generated by the task via a dynamic query.  
          An example is available in [Querying pre-made embeddings](../../../ai-integration/vector-search/vector-search-using-dynamic-query.mdx#querying-pre-made-embeddings-generated-by-tasks).
        * When indexing the embeddings generated by the task.  
          An example is available in [Indexing pre-made text-embeddings](../../../ai-integration/vector-search/vector-search-using-static-index.mdx#indexing-pre-made-text-embeddings).
        * In documents in the [Embeddings collection](../../../ai-integration/generating-embeddings/embedding-collections.mdx#the-embeddings-collection),  
          where the task identifier is used to identify the origin of each embedding.
     
        See how this identifier is used in the [Embeddings collection](../../../ai-integration/generating-embeddings/embedding-collections.mdx#the-embeddings-collection)
        documents that reference the generated embeddings.  
  
  3. **Regenerate**  
     Click "Regenerate" to automatically create an identifier based on the task name.
  4. **Task state**  
     Enable/Disable the task.
  5. **Responsible node**  
     Select a node from the [Database group](../../../studio/database/settings/manage-database-group.mdx) to be the responsible node for this task.
  6. **Connection string**  
     Select a previously defined [AI connection string](../../../ai-integration/connection-strings/overview.mdx) or create a new one.
  7. **Enable document expiration**  
     This toggle appears only if the [Document expiration feature](../../../studio/database/settings/document-expiration.mdx) is Not enabled in the database.
     Enabling document expiration ensures that embeddings in the `@embeddings-cache` collection are automatically deleted when they expire.
  8. **Save**  
     Click _Save_ to store the task definition or _Cancel_.

* **Define the embeddings source - using PATHS**:
  
    ![Create embeddings generation task - source by paths](../assets/add-ai-task-4.png)

  1. **Collection**  
     Enter or select the source document collection from the dropdown.
  2. **Embeddings source**  
     Select `Paths` to define the source content by document properties.
  3. **Path configuration**  
     Specify which document properties to extract text from, and how the text should be chunked into embeddings. 

     * **Source text path**  
       Enter the property name from the document that contains the text for embedding generation.  
     * **Chunking method**  
       Select the method for splitting the source text into chunks.  
       Learn more in [Chunking methods and tokens](../../../ai-integration/generating-embeddings/embeddings-generation-task.mdx#chunking-methods-and-tokens).  
     * **Max tokens per chunk**  
       Enter the maximum number of tokens allowed per chunk (this depends on the service provider).  
     * **Overlap tokens**  
       Enter the number of tokens to repeat at the start of each chunk from the end of the previous one.  
       This helps preserve context between chunks by carrying over some tokens from one to the next.  
       Applies only to the _"Plain Text: Split Paragraphs"_ and _"Markdown: Split Paragraphs"_ chunking methods.

  4. **Add path configuration**  
     Click to add the specified to the list.
  5. **List of paths**  
     This table displays the document properties you added for embedding generation.

* **Define the embeddings source - using SCRIPT**:
  
    ![Create embeddings generation task - source by script](../assets/add-ai-task-4-script.png)

  1. **Embeddings source**  
     Select `Script` to define the source content and chunking methods using a JavaScript script.
  2. **Script**  
     Refer to section [Chunking methods and tokens](../../../ai-integration/generating-embeddings/embeddings-generation-task.mdx#chunking-methods-and-tokens) for available JavaScript methods.
  3. **Default chunking method**  
     The selected chunking method will be used by default when no method is specified in the script.  
     e.g., when the script contains: `Name: this.Name`.
  4. **Default max tokens per chunk**:  
     Enter the default value to use when no specific value is set for the chunking method in the script.  
     This is the maximum number of tokens allowed per chunk (depends on the service provider).
  5. **Default overlap tokens**  
     Enter the default value to use when no specific value is set for the chunking method in the script.  
     This is the number of tokens to repeat at the start of each chunk from the end of the previous one.  
     Applies only to the _"Plain Text: Split Paragraphs"_ and _"Markdown: Split Paragraphs"_ chunking methods. 

* **Define quantization and expiration -  
  for the generated embeddings from the source documents**:

    ![Create embeddings generation task - quantization and expiration](../assets/add-ai-task-5.png)

  1. **Quantization**  
     Select the quantization method that RavenDB will apply to embeddings received from the service provider.  
     Available options:  
     * Single (no quantization)
     * Int8
     * Binary
  2. **Embeddings cache expiration**  
     Set the expiration period for documents stored in the `@embeddings-cache` collection.  
     These documents contain embeddings generated from the source documents, serving as a cache for these embeddings.  
     The default initial period is `90` days. This period may be extended when the source documents change.  
     Learn more in [The embeddings cache collection](../../../ai-integration/generating-embeddings/embedding-collections.mdx#the-embeddings-cache-collection).
  3. **Regenerate embeddings**  
     This toggle is visible only when editing an existing task.  
     Toggle ON to regenerate embeddings for all documents in the collection, as specified by the _Paths_ or _Script_.

* **Define chunking method & expiration -  
  for the embedding generated from a search term in a vector search query**:

    ![Create embeddings generation task - for the query](../assets/add-ai-task-6.png)

  1. **Querying**  
     This label indicates that this section configures parameters only for embeddings  
     generated by the task for **search terms in vector search queries**.
  2. **Chunking method**  
     Select the method for splitting the search term into chunks.  
     Learn more in [Chunking methods and tokens](../../../ai-integration/generating-embeddings/embeddings-generation-task.mdx#chunking-methods-and-tokens).
  3. **Max tokens per chunk**  
     Enter the maximum number of tokens allowed per chunk (this depends on the service provider).
  4. **Overlap tokens**  
     Enter the number of tokens to repeat at the start of each chunk from the end of the previous one.  
     This helps preserve context between chunks by carrying over some tokens from one to the next.  
     Applies only to the _"Plain Text: Split Paragraphs"_ and _"Markdown: Split Paragraphs"_ chunking methods.
  5. **Embeddings cache expiration**  
     Set the expiration period for documents stored in the `@embeddings-cache` collection.  
     These documents contain embeddings generated from the search terms, serving as a cache for these embeddings.  
     The default period is `14` days. Learn more in [The embeddings cache collection](../../../ai-integration/generating-embeddings/embedding-collections.mdx#the-embeddings-cache-collection).

</Panel>

<Panel heading="Configuring an embeddings generation task - from the Client API">

<ContentFrame>

#### Configure an embeddings generation task - define source using PATHS:

<TabItem>
```js
// Define a connection string that will be used in the task definition:
// ====================================================================
const connectionString = new AiConnectionString();

connectionString.name = "ConnectionStringToOpenAi";
connectionString.identifier = 'id-for-open-ai-connection-string';
connectionString.openAiSettings = new OpenAiSettings(
    "your-api-key",
    "https://api.openai.com/v1",
    "text-embedding-3-small");

// Deploy the connection string to the server:
// ===========================================
const putConnectionStringOp = new PutConnectionStringOperation(connectionString);
const putConnectionStringResult = await documentStore.maintenance.send(putConnectionStringOp);

// Define the embeddings generation task:
// ======================================
const embeddingsTaskConfiguration = new EmbeddingsGenerationConfiguration();

// General info:    
embeddingsTaskConfiguration.name = "GetEmbeddingsFromOpenAI";
embeddingsTaskConfiguration.identifier = "id-for-task-open-ai";
embeddingsTaskConfiguration.connectionStringName = connectionString.name;
embeddingsTaskConfiguration.disabled = false;    
    
// Embeddings source & chunking methods - using PATHS configuration:
embeddingsTaskConfiguration.collection = "Categories";    
embeddingsTaskConfiguration.embeddingsPathConfigurations = [
    {
        path: "Name",
        chunkingOptions: {
            chunkingMethod: "PlainTextSplit",
            maxTokensPerChunk: 2048
        }
    },
    {
        path: "Description",
        chunkingOptions: {
            chunkingMethod: "PlainTextSplitParagraphs",
            maxTokensPerChunk: 2048,
    
            // 'overlapTokens' is only applicable when 'chunkingMethod' is 
            // 'PlainTextSplitParagraphs' or 'MarkDownSplitParagraphs'
            overlapTokens: 128
        }
    }
];    

// Quantization & expiration -
// for embeddings generated from source documents: 
embeddingsTaskConfiguration.quantization = "Single";
embeddingsTaskConfiguration.embeddingsCacheExpiration = 90 * 24 * 60 * 60 * 1000; // 90 days    
    
// Chunking method and expiration -
// for the embeddings generated from search term in vector search query:
embeddingsTaskConfiguration.chunkingOptionsForQuerying = {
    chunkingMethod: "PlainTextSplit",
    maxTokensPerChunk: 2048
}

embeddingsTaskConfiguration.embeddingsCacheExpiration = 14 * 24 * 60 * 60 * 1000; // 14 days

// Deploy the embeddings generation task to the server:
// ====================================================
const addEmbeddingsGenerationOp =
    new AddEmbeddingsGenerationOperation(embeddingsTaskConfiguration);
const addAiIntegrationTaskResult = 
    await documentStore.maintenance.send(addEmbeddingsGenerationOp);
```
</TabItem>

</ContentFrame>

<ContentFrame>

#### Configure an embeddings generation task - define source using SCRIPT:

* To configure the source content using a script -  
  use the `embeddingsTransformation` object instead of the `embeddingsPathConfigurations` object.

* The rest of the configuration properties are the same as in the example above.

* Call `embeddings.generate(object)` within the script and apply the appropriate text-splitting methods to each field inside the object.
  Each KEY in the object represents a document field, and the VALUE is a text-splitting function that processes the field's content before generating embeddings.

* These methods ensure that the text chunks derived from document fields stay within the token limits required by the provider, preventing request rejection.
  Learn more in [Chunking methods and tokens](../../../ai-integration/generating-embeddings/embeddings-generation-task.mdx#chunking-methods-and-tokens).
 
* For example:  

<TabItem>
```js
// Source collection:
embeddingsTaskConfiguration.collection = "Categories";

Use 'embeddingsTransformation':
embeddingsTaskConfiguration.embeddingsTransformation = {
    script: `
        embeddings.generate({

            // Process the document 'Name' field using method text.split().
            // The text content will be split into chunks of up to 2048 tokens.
            Name: text.split(this.Name, 2048),
    
            // Process the document 'Description' field using method text.splitParagraphs().
            // The text content will be split into chunks of up to 2048 tokens.
            // 128 overlapping tokens will be repeated at the start of each chunk
            // from the end of the previous one.
            Description: text.splitParagraphs(this.Description, 2048, 128)
        });`
}
```
</TabItem>

* If no chunking method is provided in the script, you can set default values as follows:  

<TabItem>
```js
embeddingsTaskConfiguration.collection = "Categories";
    
embeddingsTaskConfiguration.embeddingsTransformation = {
    script: `
        embeddings.generate({

            // No chunking method is specified here
            Name: this.Name,
            Description: this.Description
        });`,
        
    // Specify the default chunking options to use in the script
    chunkingOptions: {
        chunkingMethod: "PlainTextSplit",
        maxTokensPerChunk: 2048
    }
}
```
</TabItem>

</ContentFrame>

</Panel>

<Panel heading="Chunking methods and tokens">

**Tokens and processing limits**:  

* A token is the fundamental unit that Large Language Models (LLMs) use to process text.  
  AI service providers that generate embeddings from text enforce token limits for each processed text part.  
  If a text exceeds the providerâ€™s limit, it may be truncated or rejected.

**Using chunking methods**:  

* To handle lengthy text, you can define chunking strategies in the task definition and specify the desired number of tokens per chunk.
  Chunking splits large input texts into smaller, manageable chunks, each containing no more than the specified maximum number of tokens.  

* The maximum number of tokens per chunk depends on the AI service provider and the specific model defined in the [connection string](../../../ai-integration/connection-strings/overview.mdx).
  While RavenDB does not tokenize text, it estimates the number of tokens for chunking purposes by dividing the text length by 4.

* The AI provider generates a single embedding for each chunk.  
  Depending on the maximum tokens per chunk setting, a single input text may result in multiple embeddings.

**Available chunking methods**:  

RavenDB offers several chunking methods that can be applied per source type.  
These methods determine how input text is split before being sent to the provider.  

<Admonition type="note" title="">

* `PlainText: Split`  
  Splits a plain text string into multiple chunks based on the specified maximum token count.  
  Estimates token lengths based on an average of 4 characters per token and applies a 0.75 ratio to determine chunk sizes.
  Ensures that words are not split mid-way when forming chunks.  

     **Applies to**:  
     Fields containing plain text strings.  
     **Return Value**:   
     A list of text chunks (strings), where each chunk approximates the specified maximum token count without breaking words.

* `PlainText: Split Lines`  
  Uses the Semantic Kernel _SplitPlainTextLines_ method.  
  Splits a plain text string into individual lines based on line breaks and whitespace while ensuring that each line does not exceed the specified maximum token limit.  

     **Applies to**:  
     Fields containing an array of plain text strings.  
     **Return value**:  
     A list of text segments (lines) derived from the original input, preserving line structure while ensuring token constraints.

* `PlainText: Split Paragraphs`  
  Uses the Semantic Kernel _SplitPlainTextParagraphs_ method.  
  Combines consecutive lines to form paragraphs while ensuring each paragraph is as complete as possible without exceeding the specified token limit.  
  Optionally, set an overlap between chunks using the _overlapTokens_ parameter, which repeats the last _n_ tokens from one chunk at the start of the next.
  This helps preserve context continuity across paragraph boundaries.  
  
     **Applies to**:  
     Fields containing an array of plain text strings.  
     **Return value**:   
     A list of paragraphs, where each paragraph consists of grouped lines that preserve readability without exceeding the token limit.

* `Markdown: Split Lines`   
  Uses the Semantic Kernel _SplitMarkDownLines_ method.  
  Splits markdown content into individual lines at line breaks while ensuring that each line remains within the specified token limit.
  Preserves markdown syntax, ensuring each line remains an independent, valid segment.  
  
     **Applies to**:  
     Fields containing strings with markdown content.  
     **Return value**:   
     A list of markdown lines, each respecting the token limit while maintaining the original formatting.

* `Markdown: Split Paragraphs`  
  Uses the Semantic Kernel _SplitMarkdownParagraphs_ method.  
  Groups lines into coherent paragraphs at designated paragraph breaks while ensuring each paragraph remains within the specified token limit.
  Markdown formatting is preserved.  
  Optionally, set an overlap between chunks using the _overlapTokens_ parameter, which repeats the last _n_ tokens from one chunk at the start of the next.
  This helps preserve context continuity across paragraph boundaries. 
  
  
     **Applies to**:  
     Fields containing an array of strings with markdown content.  
     **Return value**:  
     A list of markdown paragraphs, each respecting the token limit and maintaining structural integrity.   

* `HTML: Strip`  
  Removes HTML tags from the content and splits the resulting plain text into chunks based on a specified token limit.  

     **Applies to**:  
     Fields containing strings with HTML.   
     **Return value**:  
     A list of text chunks derived from the stripped content, ensuring each chunk remains within the token limit.

</Admonition>
**Chunking method syntax for the JavaScript scripts**:  

<TabItem>
```js
// Available text-splitting methods:
// =================================

// Plain text methods:
text.split(text | [text], maxTokensPerLine);
text.splitLines(text | [text], maxTokensPerLine);
text.splitParagraphs(line | [line], maxTokensPerLine, overlapTokens?);

// Markdown methods:
markdown.splitLines(text | [text], maxTokensPerLine);
markdown.splitParagraphs(line | [line], maxTokensPerLine, overlapTokens?);

// HTML processing:
html.strip(htmlText | [htmlText], maxTokensPerChunk);
```
</TabItem>

| Parameter                                | Type      | Description                                                       |
|------------------------------------------|-----------|------------------------------------------------------------------ |
| **text**                                 | `string`  | A plain text or markdown string to split.                         |
| **line**                                 | `string`  | A single line or paragraph of text.                               |
| **[text] / [line]**                      | `string[]`| An array of text or lines to split into chunks.                   |
| **htmlText**                             | `string`  | A string containing HTML content to process.                      |
| **maxTokensPerChunk / maxTokensPerLine** | `number`  | The maximum number of tokens allowed per chunk.<br/>Default is `512`. |
| **overlapTokens**                        | `number` (optional) | The number of tokens to overlap between consecutive chunks. Helps preserve context continuity across chunks (e.g., between paragraphs).<br/>Default is `0`. |

</Panel>

<Panel heading="Syntax">

`EmbeddingsGenerationConfiguration`

<TabItem>
```js
// The 'EmbeddingsGenerationConfiguration' class extends 'EtlConfiguration'
// and provides the following specialized configurations for the embeddings generation task:
// =========================================================================================

class EmbeddingsGenerationConfiguration 
{
    // The identifier of the embeddings generation task. 
    identifier; // string
    
    // The name of the source collection from which documents are processed
    // for embeddings generation.
    collection; // string
    
    // A list of properties inside documents that contain text to be embedded, 
    // along with their chunking settings. 
    embeddingsPathConfigurations; // object[]
    
    // An object that contains a script defining the transformations and processing applied 
    // to the source text before generating embeddings.  
    embeddingsTransformation; // object
    
    // The quantization type for the generated embeddings.
    // Can be: "Single" | "Int8" | "Binary" 
    // Cannot be "Text". Default is "Single".
    quantization; // string
    
    // The chunking method, maximum token limit, 
    // and overlap tokens used when processing search terms in vector search queries. 
    chunkingOptionsForQuerying; // object
    
    // The expiration period for documents in the 'embedding cache collection' that contain 
    // embeddings generated from source documents.
    embeddingsCacheExpiration; // number
    
    // The expiration period for documents in the 'embedding cache collection' that contain
    // embeddings generated from search terms in vector search queries. 
    embeddingsCacheForQueryingExpiration; // number
}
```
</TabItem>
    
`EmbeddingPathConfiguration`  
    
<TabItem>
```js
// The embedding path configuration object    
{
    // JSON path to the document field.
    path // string
    
    // Chunking configuration for this specific field.
    chunkingOptions // object
}
```    
</TabItem>
    
`ChunkingOptions` 
    
<TabItem>
```js    
// The chunking options object
{
    // The method to use for splitting text into chunks. Can be:
    // "PlainTextSplit", "PlainTextSplitLines", "PlainTextSplitParagraphs", "MarkDownSplitLines", 
    // "MarkDownSplitParagraphs", "HtmlStrip"
    // Default is "PlainTextSplit"
    chunkingMethod, // string 
    
    // Maximum number of tokens per chunk.
    // Default is 512
    maxTokensPerChunk, // number 
    
    // Number of tokens to overlap between consecutive chunks.
    // 'OverlapTokens' is only applicable when chunkingMethod is 
    // 'PlainTextSplitParagraphs' or 'MarkDownSplitParagraphs'.
    // Default is 0
    overlapTokens // number
}
```
</TabItem>
    
`EmbeddingsTransformation`
    
<TabItem>
```js
// The embeddings transformation object
{
    // JavaScript transformation script.
    // Must call embeddings.generate() function.    
    script; // string
    
    // Chunking configuration for the transformation output.
    chunkingOptions; // object
}
```
</TabItem>
    
#### Deploying the embeddings generation task:

<TabItem>
```js
const result = AddEmbeddingsGenerationOperation(configuration);    
```
</TabItem>

| Parameter         | Type                                | Description                                   |
|-------------------|-------------------------------------|-----------------------------------------------|
| **configuration** | `EmbeddingsGenerationConfiguration` | The embeddings generation task configuration. |    
    
</Panel>